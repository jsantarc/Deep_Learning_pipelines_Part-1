{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# King County House Price Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was created for the IBM meetup on February 13 2020. It contains neural networks trained to predict house prices in King County, Washington, USA based on features of the house. They are trained using different preprocessing methods like PCA and Standardization. They are also trained using different Neural Networks with one featuring batch normalization. We will explore the benefits of data preprocessing on our neural networks and how they affect performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data, Creating NN, and Training Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Pillow==6.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Restart the kernel using the options on the top left***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pylab as plt\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset we will use and the already trained models to save time instead of training from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/king_county_house_data.csv\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/pca_model.pt\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/regular_model.pt\n",
    "!wget https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/meet_up/12.02.2020/standardized_model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the csv into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('king_county_house_data.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a PyTorch Dataset object to store our dataframe values in. We create 3 type of data one for data with PCA, Standardization, and regular data with no preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data(Dataset):\n",
    "    \n",
    "    def __init__(self, test=False, regular=False, pca=False, standard=False):\n",
    "        \n",
    "        X = df.drop(columns='price')\n",
    "        Y = df['price']\n",
    "        # Split the data into training data and testing data using a 80/20 split\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=.2, random_state=1)\n",
    "        \n",
    "        x_train = x_train.astype(float)\n",
    "        x_test = x_test.astype(float)\n",
    "        \n",
    "        if (regular):\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test.values)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train.values)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "                \n",
    "        elif (pca):\n",
    "            \n",
    "            # Performs PCA on the data\n",
    "            pca = PCA(whiten=True)\n",
    "            x_train = pca.fit_transform(x_train)\n",
    "            x_test = pca.transform(x_test)\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "                \n",
    "        else:\n",
    "            \n",
    "            # Performs Standardization on the data\n",
    "            standard = preprocessing.StandardScaler()\n",
    "            x_train = standard.fit_transform(x_train)\n",
    "            x_test = standard.transform(x_test)\n",
    "            \n",
    "            if (test):\n",
    "                self.x = torch.FloatTensor(x_test)\n",
    "                self.y = torch.FloatTensor(y_test.values).view(-1,1)\n",
    "            else:\n",
    "                self.x = torch.FloatTensor(x_train)\n",
    "                self.y = torch.FloatTensor(y_train.values).view(-1,1)\n",
    "\n",
    "    # Getter\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the Neural Networks we will use. They are both 4 layered networks with 1 input layer, 1 output layer, and 2 hidden layers. THe number of nodes in each layer is 2/3 of the input layer except the output because we are doing regression. They both use relu as the activation function.\n",
    "\n",
    "The difference is that one uses Batch Normalization and one does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalizationNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super(BatchNormalizationNN, self).__init__()\n",
    "        # The dataset has 18 features\n",
    "        self.linear1 = nn.Linear(18, 12)\n",
    "        self.b1 = nn.BatchNorm1d(12)\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.b2 = nn.BatchNorm1d(8)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "    # Prediction    \n",
    "    def forward(self, x): \n",
    "        y = self.b1(torch.relu(self.linear1(x)))\n",
    "        y = self.b2(torch.relu(self.linear2(y)))\n",
    "        y = self.linear3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularNN(nn.Module):\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        super(RegularNN, self).__init__()\n",
    "        # The dataset has 18 features\n",
    "        self.linear1 = nn.Linear(18, 12)\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.linear3 = nn.Linear(8, 1)\n",
    "\n",
    "    # Prediction    \n",
    "    def forward(self, x): \n",
    "        y = torch.relu(self.linear1(x)) \n",
    "        y = torch.relu(self.linear2(y))\n",
    "        y = self.linear3(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function performs the training of the models. It is given the training data, testing data, criterion (loss function), model(Neural Network), optimizer, and epochs (Number of times the model is trained on the entire dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, test_loader, criterion, model, optimizer, epochs):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print(epoch)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            # Set the model in training mode so it updates values in Batch Normalization\n",
    "            model.train()\n",
    "            # Clears the optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Calculates the prediction the model gives\n",
    "            z = model(x)\n",
    "            # Calculates the loss between the prediction and the actual value\n",
    "            loss = criterion(z, y)\n",
    "            # Calcualates the partial deriviative for each parameter of the Neural Network\n",
    "            loss.backward()\n",
    "            # Adjusts the parameters according to the optimizer\n",
    "            optimizer.step()\n",
    "            total_loss = total_loss + loss.item()\n",
    "        \n",
    "        train_loss.append(total_loss/(len(train_loader.dataset)/train_loader.batch_size))\n",
    "        print(total_loss/(len(train_loader.dataset)/train_loader.batch_size))\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            # Set the model in evaluation mode so we do not train the Batch Normalization while evaluating our model\n",
    "            model.eval()\n",
    "            z = model(x)\n",
    "            loss = criterion(z, y)\n",
    "            total_loss = total_loss + loss.item()\n",
    "           \n",
    "        test_loss.append(total_loss/len(test_loader.dataset))\n",
    "        print(total_loss/len(test_loader.dataset))\n",
    "        \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularModel = RegularNN()\n",
    "checkpoint = torch.load('regular_model.pt')\n",
    "RegularModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "RegularModel.eval()\n",
    "regular_train_loss = checkpoint['train_loss']\n",
    "regular_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCAModel = BatchNormalizationNN()\n",
    "checkpoint = torch.load('pca_model.pt')\n",
    "PCAModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "PCAModel.eval()\n",
    "pca_train_loss = checkpoint['train_loss']\n",
    "pca_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StandardModel = BatchNormalizationNN()\n",
    "checkpoint = torch.load('standardized_model.pt')\n",
    "StandardModel.load_state_dict(checkpoint['model_state_dict'])\n",
    "StandardModel.eval()\n",
    "stan_train_loss = checkpoint['train_loss']\n",
    "stan_test_loss = checkpoint['test_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the sections below is code to train the models with each type of preprocessing. The training code is saved a raw so it does not accidentally get run as it would take a lot of time to train. The loss function used is SmoothL1Loss and in its current setup it gives the absolute value of the error between the prediction and actual value. The optimizer used is Adadelta, we use this because we do not need to worry about initializing or updating the rate at which the Neural Network learns. If you would like to experiment with the Neural Networks please change the type of the training block to code using the options at the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d_train = data(regular=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(regular=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "RegularModel = RegularNN()\n",
    "optimizer = torch.optim.Adadelta(RegularModel.parameters())\n",
    "regular_train_loss, regular_test_loss = train(train_loader, test_loader, criterion, RegularModel, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regular_train_loss)\n",
    "plt.title(\"Regular Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss \")\n",
    "plt.show()\n",
    "print(regular_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(regular_test_loss)\n",
    "plt.title(\"Regular Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss \")\n",
    "plt.show()\n",
    "print(regular_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 500,\n",
    "    'model_state_dict': RegularModel.state_dict(),\n",
    "    'train_loss': regular_train_loss,\n",
    "    'test_loss': regular_test_loss\n",
    "}, 'regular_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d_train = data(pca=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(pca=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "PCAModel = BatchNormalizationNN()\n",
    "optimizer = torch.optim.Adadelta(PCAModel.parameters())\n",
    "pca_train_loss, pca_test_loss = train(train_loader, test_loader, criterion, PCAModel, optimizer, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_train_loss)\n",
    "plt.title(\"PCA Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(pca_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca_test_loss)\n",
    "plt.title(\"PCA Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(pca_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 60,\n",
    "    'model_state_dict': PCAModel.state_dict(),\n",
    "    'train_loss': pca_train_loss,\n",
    "    'test_loss': pca_test_loss\n",
    "}, 'pca_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized NN"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "d_train = data(standard=True, test=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=d_train, batch_size=500)\n",
    "d_test = data(standard=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "criterion = nn.SmoothL1Loss()\n",
    "StandardModel = BatchNormalizationNN()\n",
    "optimizer = torch.optim.Adadelta(StandardModel.parameters())\n",
    "stan_train_loss, stan_test_loss = train(train_loader, test_loader, criterion, StandardModel, optimizer, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stan_train_loss)\n",
    "plt.title(\"Standardization Train\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(stan_train_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stan_test_loss)\n",
    "plt.title(\"Standardization Test\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.show()\n",
    "print(stan_test_loss[-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save({\n",
    "    'epochs': 250,\n",
    "    'model_state_dict': StandardModel.state_dict(),\n",
    "    'train_loss': stan_train_loss,\n",
    "    'test_loss': stan_test_loss\n",
    "}, 'standardized_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will allow you to see the predicted price, actual price, and error for each house in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss()\n",
    "limit = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(regular=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    RegularModel.eval()\n",
    "    z = RegularModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(pca=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    PCAModel.eval()\n",
    "    z = PCAModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_test = data(standard=True, test=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=d_test)\n",
    "\n",
    "for i, (x, y) in enumerate(test_loader):\n",
    "    StandardModel.eval()\n",
    "    z = StandardModel(x)\n",
    "    loss = criterion(z, y)\n",
    "    print(i)\n",
    "    print('Predicted: ',z.item())\n",
    "    print('Actual: ', y.item())\n",
    "    print('Loss: ', loss.item())\n",
    "    if (i == limit):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created By: Azim Hirjani"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
